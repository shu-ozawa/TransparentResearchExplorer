import os
import asyncio
import logging
import sys
import re # For checking log messages

# Add backend to sys.path
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '.')))

from backend.app.clients.gemini_client import GeminiClient
from backend.api.endpoints.research_tree import _generate_research_plan

# --- Logging Setup ---
# Create a list handler to capture log records
log_capture_handler = logging.StreamHandler(sys.stdout) # Print logs to stdout for easy capture
log_capture_handler.setLevel(logging.INFO)
formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
log_capture_handler.setFormatter(formatter)

# Get the root logger and add the handler
# Also configure the root logger level if necessary, or configure specific loggers
logging.basicConfig(level=logging.INFO, handlers=[log_capture_handler], force=True)

# Ensure specific loggers are also at INFO if they have different default levels
logger_research_tree = logging.getLogger('backend.api.endpoints.research_tree')
logger_research_tree.setLevel(logging.INFO)
# if using a different logger for gemini client, configure it too
# logger_gemini_client = logging.getLogger('backend.app.clients.gemini_client')
# logger_gemini_client.setLevel(logging.INFO)


async def run_test_scenario(scenario_name: str, natural_query: str, max_q: int, gemini_client: GeminiClient):
    print(f"\n\n--- Testing Scenario: {scenario_name} ---")
    
    # Clear previous log captures if you were storing them in a list
    # For stdout, this is not strictly necessary but good practice if you had a list-based handler
    
    print(f"Input Natural Query: {natural_query}")
    print(f"Max Queries Param: {max_q}")

    # Call the function
    research_goal, queries = await _generate_research_plan(natural_query, gemini_client, max_q)
    
    print(f"\n--- Results for Scenario: {scenario_name} ---")
    print(f"Input Natural Query (Verification): {natural_query}")
    print(f"Returned Research Goal: {research_goal}")
    print(f"Number of Queries Generated: {len(queries)}")

    # Verification Steps
    print("\n--- Verification ---")
    # 1. Assert research_goal == natural_query
    if research_goal == natural_query:
        print("✅ PASSED: research_goal == natural_query")
    else:
        print(f"❌ FAILED: research_goal ({research_goal}) != natural_query ({natural_query})")

    # 2. Assert queries list is not empty
    if queries:
        print("✅ PASSED: Queries list is not empty.")
    else:
        print("❌ FAILED: Queries list is empty.")

    # 3. Assert that if LLM response indicates multiple queries, parsed list matches
    # This requires inspecting logs. For this script, we'll make a simpler check:
    # If queries were generated (not empty), and it's not the fallback, it's a good sign.
    # A more robust check would parse the 'Processed LLM Response for Parsing:' log.
    
    is_fallback = (len(queries) == 1 and queries[0][0] == natural_query and queries[0][1] == "Original query")
    
    if not queries: # Already handled by "list is empty"
        pass
    elif is_fallback:
        print("⚠️ WARNING: Queries list is the fallback (original query). This might be expected if LLM returned no parsable queries. Check 'Processed LLM Response' log.")
    elif len(queries) < 2 and len(queries) > 0 : # LLM returned only one query, but it's not the fallback
         print(f"ℹ️ INFO: Only one query was generated by the LLM and parsed: Query: '{queries[0][0]}', Desc: '{queries[0][1]}'")
    else: # Multiple queries generated
        print(f"✅ PASSED: Multiple queries ({len(queries)}) generated and parsed (not fallback).")


    print("\nGenerated Queries Details:")
    if not queries:
        print("  No queries to display.")
    else:
        for i, (q, d) in enumerate(queries):
            print(f"  Query {i+1}: {q}")
            print(f"    Description: {d}")
            # 4. Basic check for English (relies on visual inspection of output)
            is_english = all(ord(char) < 128 for char in q + d) # Simple ASCII check
            print(f"    Appears English: {is_english}")
    print(f"--- End of Scenario: {scenario_name} ---\n")


async def main():
    # Set the API key
    api_key_value = "AIzaSyBGkZZT8yLLxDH_0p9cPxRIaRunH3vNI-c"
    os.environ["GEMINI_API_KEY"] = api_key_value
    
    api_key = os.getenv("GEMINI_API_KEY")
    if not api_key:
        logging.error("CRITICAL: GEMINI_API_KEY environment variable not set after attempting to set it.")
        return
    logging.info(f"GEMINI_API_KEY set: {api_key[:5]}...{api_key[-4:]}")

    gemini_client = GeminiClient()
    logging.info("GeminiClient instantiated.")

    # Scenario 1: User's Japanese Input
    natural_query_jp = "LLMを用いた材料の合成可能性の説明とその評価"
    max_q_jp = 5
    await run_test_scenario("Japanese Input", natural_query_jp, max_q_jp, gemini_client)

    # Scenario 2: English Input (Regression Test)
    natural_query_en = "To understand the methodologies, applications, and evaluation strategies for fine-tuning Large Language Models (LLMs) to perform effectively within specific domains."
    max_q_en = 5
    await run_test_scenario("English Input (Regression)", natural_query_en, max_q_en, gemini_client)

if __name__ == "__main__":
    asyncio.run(main())
